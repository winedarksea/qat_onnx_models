{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLO26 Nano Training for MIAP Person Detection\n",
    "\n",
    "This notebook performs the following steps:\n",
    "1.  **Setup**: Installs necessary libraries and connects to Google Drive and Google Cloud for data access.\n",
    "2.  **Data Preparation**: \n",
    "    - Downloads annotations for the MIAP (person subset of Google Open Dataset) from a GCS bucket.\n",
    "    - Downloads the corresponding images from the public Open Images dataset bucket.\n",
    "    - Filters out images containing only very small bounding boxes (width or height < 6 pixels).\n",
    "    - Converts the dataset into the YOLO format required by Ultralytics.\n",
    "3.  **Training**: Trains a `yolo26n` (nano) model on the prepared dataset.\n",
    "4.  **Export**: \n",
    "    - Exports the trained model to ONNX format, including preprocessing and NMS post-processing.\n",
    "    - Creates a Float32 version.\n",
    "    - Creates an INT8 quantized version using static calibration.\n",
    "5.  **Save & Verify**: Saves the final models to Google Drive and runs a quick verification with ONNX Runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fake interaction (cmd option I, console)\n",
    "```js\n",
    "function KeepAlive() {\n",
    "    console.log(\"Interaction simulated\");\n",
    "    // This targets the actual button hidden inside the shadow root\n",
    "    const connectButton = document.querySelector(\"#top-toolbar > colab-connect-button\")\n",
    "        ?.shadowRoot?.querySelector(\"#connect\");\n",
    "    \n",
    "    if (connectButton) {\n",
    "        connectButton.click();\n",
    "    } else {\n",
    "        // Fallback: Click the comment button or a generic toolbar item\n",
    "        document.querySelector(\"colab-toolbar-button#comments\")?.click();\n",
    "    }\n",
    "}\n",
    "setInterval(KeepAlive, 60000); // Runs every 1 minute\n",
    "```\n",
    "or this version with randomness\n",
    "```js\n",
    "function KeepAlive() {\n",
    "    // Generate a random delay between 1 and 3 minutes (in milliseconds)\n",
    "    const min = 1 * 60 * 1000;\n",
    "    const max = 3 * 60 * 1000;\n",
    "    const randomDelay = Math.floor(Math.random() * (max - min + 1)) + min;\n",
    "\n",
    "    console.log(`Interaction simulated. Next check in: ${Math.round(randomDelay/1000)}s`);\n",
    "\n",
    "    // Target the button inside the Shadow DOM\n",
    "    const connectButton = document.querySelector(\"#top-toolbar > colab-connect-button\")\n",
    "        ?.shadowRoot?.querySelector(\"#connect\");\n",
    "    \n",
    "    if (connectButton) {\n",
    "        connectButton.click();\n",
    "    } else {\n",
    "        // Fallback: Click the 'Comment' icon to register activity\n",
    "        document.querySelector(\"colab-toolbar-button#comments\")?.click();\n",
    "    }\n",
    "\n",
    "    // Schedule the next execution with the new random delay\n",
    "    setTimeout(KeepAlive, randomDelay);\n",
    "}\n",
    "\n",
    "// Start the first cycle\n",
    "KeepAlive();\n",
    "```\n",
    "Run `caffeinate` on mac terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install ultralytics onnx onnxruntime onnxsim pandas gcsfs tqdm -q\n",
    "!pip install uv\n",
    "\n",
    "# Use uv pip install with the --system flag to install into the current environment\n",
    "!uv pip install ultralytics onnx onnxruntime onnxsim pandas gcsfs tqdm --system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup and Authentication\n",
    "\n",
    "Mount Google Drive to save the final models and authenticate with Google Cloud to access the dataset annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive, auth\n",
    "import os\n",
    "\n",
    "print(\"Mounting Google Drive...\")\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(\"Authenticating with Google Cloud...\")\n",
    "auth.authenticate_user()\n",
    "\n",
    "# Define a directory in your Google Drive to save the models\n",
    "GDRIVE_SAVE_DIR = '/content/drive/MyDrive/miap_yolov26_models'\n",
    "os.makedirs(GDRIVE_SAVE_DIR, exist_ok=True)\n",
    "print(f\"Models will be saved to: {GDRIVE_SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import numpy as np\n",
    "from onnx import helper, numpy_helper, TensorProto\n",
    "import onnxruntime as ort\n",
    "from onnxruntime.quantization import quantize_static, CalibrationDataReader, QuantFormat, QuantType, CalibrationMethod\n",
    "\n",
    "def embed_uint8_preprocess_into_onnx(in_onnx, out_onnx, imgsz, input_scale=1.0/255.0):\n",
    "    \"\"\"\n",
    "    Embeds uint8 normalization and resize into the ONNX graph.\n",
    "    Input: [1, 3, H, W] uint8\n",
    "    Output: [1, 3, imgsz, imgsz] float32\n",
    "    \"\"\"\n",
    "    model = onnx.load(in_onnx)\n",
    "    orig_in_name = model.graph.input[0].name\n",
    "    \n",
    "    new_input_name = \"images_uint8\"\n",
    "    # Use dynamic H and W to allow input of any size, which will be resized to imgsz\n",
    "    new_input = helper.make_tensor_value_info(new_input_name, TensorProto.UINT8, [1, 3, \"Height\", \"Width\"])\n",
    "    \n",
    "    cast_out = f\"{orig_in_name}__cast_f32\"\n",
    "    scaled_out = f\"{orig_in_name}__scaled\"\n",
    "    resize_out = orig_in_name\n",
    "\n",
    "    nodes = [\n",
    "        helper.make_node(\"Cast\", inputs=[new_input_name], outputs=[cast_out], to=TensorProto.FLOAT, name=\"Preprocess_Cast\"),\n",
    "        helper.make_node(\"Mul\", inputs=[cast_out, \"Preprocess_Scale_Val\"], outputs=[scaled_out], name=\"Preprocess_Scale\")\n",
    "    ]\n",
    "    \n",
    "    # Preprocessing Initializers\n",
    "    scale_tensor = numpy_helper.from_array(np.array([input_scale], dtype=np.float32), name=\"Preprocess_Scale_Val\")\n",
    "    roi = numpy_helper.from_array(np.array([], dtype=np.float32), name=\"Preprocess_ROI\")\n",
    "    scales = numpy_helper.from_array(np.array([], dtype=np.float32), name=\"Preprocess_Scales\")\n",
    "    sizes = numpy_helper.from_array(np.array([1, 3, imgsz, imgsz], dtype=np.int64), name=\"Preprocess_Sizes\")\n",
    "    \n",
    "    model.graph.initializer.extend([scale_tensor, roi, scales, sizes])\n",
    "    \n",
    "    nodes.append(helper.make_node(\"Resize\", \n",
    "                                  inputs=[scaled_out, \"Preprocess_ROI\", \"Preprocess_Scales\", \"Preprocess_Sizes\"], \n",
    "                                  outputs=[resize_out], mode=\"linear\", name=\"Preprocess_Resize\"))\n",
    "\n",
    "    # Reconstruct graph\n",
    "    # Remove the old input and add the new uint8 input\n",
    "    del model.graph.input[0]\n",
    "    model.graph.input.insert(0, new_input)\n",
    "    \n",
    "    # Prepend preprocessing nodes to the graph\n",
    "    original_nodes = list(model.graph.node)\n",
    "    del model.graph.node[:]\n",
    "    model.graph.node.extend(nodes + original_nodes)\n",
    "    \n",
    "    onnx.checker.check_model(model)\n",
    "    onnx.save(model, out_onnx)\n",
    "    print(f\"Preprocessed ONNX saved to {out_onnx}\")\n",
    "    return out_onnx\n",
    "\n",
    "def finalize_onnx_for_deployment(onnx_path, imgsz):\n",
    "    \"\"\"\n",
    "    Aligns the ONNX model with the ONNX_GUIDELINES.md:\n",
    "    1. Rescale coordinate outputs to the original input resolution (uint8).\n",
    "    2. Consolidate outputs into a single [N, 7] tensor: [x1, y1, x2, y2, score, class_id, batch_idx].\n",
    "    3. Integrated NMS is expected to already be present (from export(nms=True)).\n",
    "    \"\"\"\n",
    "    model = onnx.load(onnx_path)\n",
    "    graph = model.graph\n",
    "    input_name = graph.input[0].name\n",
    "    \n",
    "    # Check if there is already an output with shape [1, N, 6] (standard Ultralytics NMS output)\n",
    "    target_out_name = None\n",
    "    for out in graph.output:\n",
    "        try:\n",
    "            shape = [d.dim_value if d.dim_value > 0 else 0 for d in out.type.tensor_type.shape.dim]\n",
    "            # Ultralytics with nms=True usually gives [1, 300, 6] or similar\n",
    "            if len(shape) == 3 and shape[0] == 1 and shape[2] == 6:\n",
    "                target_out_name = out.name\n",
    "                break\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    if not target_out_name:\n",
    "        print(f\"Warning: Could not find output with shape [1, N, 6] in {onnx_path}. Is nms=True used?\")\n",
    "        # Fallback: just rename existing outputs if they exist\n",
    "        for i, out in enumerate(model.graph.output):\n",
    "            out.name = \"detections\" if i == 0 else f\"detections_{i}\"\n",
    "        onnx.save(model, onnx_path)\n",
    "        return\n",
    "    \n",
    "    # GUIDELINE IMPLEMENTATION:\n",
    "    \n",
    "    # 1. Reshape [1, N, 6] -> [N, 6]\n",
    "    n6_name = \"Guideline_Reshaped_N6\"\n",
    "    n6_shape_const = \"Guideline_N6_Shape_Const\"\n",
    "    graph.initializer.append(numpy_helper.from_array(np.array([-1, 6], dtype=np.int64), name=n6_shape_const))\n",
    "    graph.node.append(helper.make_node(\"Reshape\", inputs=[target_out_name, n6_shape_const], outputs=[n6_name], name=\"Guideline_ReshapeN6\"))\n",
    "\n",
    "    # 2. Split [N, 6] into [N, 4], [N, 1], [N, 1]\n",
    "    b_raw, s_raw, c_raw = \"Guideline_B_Raw\", \"Guideline_S_Raw\", \"Guideline_C_Raw\"\n",
    "    split_const = \"Guideline_Split_Const\"\n",
    "    graph.initializer.append(numpy_helper.from_array(np.array([4, 1, 1], dtype=np.int64), name=split_const))\n",
    "    graph.node.append(helper.make_node(\"Split\", inputs=[n6_name, split_const], outputs=[b_raw, s_raw, c_raw], axis=1, name=\"Guideline_SplitN6\"))\n",
    "\n",
    "    # 3. Generate batch_idx [N, 1] (all zeros, Float32)\n",
    "    s_shape = \"Guideline_S_Shape\"\n",
    "    b_idx_2d = \"Guideline_BatchIdx_2d\"\n",
    "    graph.node.append(helper.make_node(\"Shape\", inputs=[s_raw], outputs=[s_shape], name=\"Guideline_GetSShape\"))\n",
    "    graph.node.append(helper.make_node(\n",
    "        \"ConstantOfShape\", \n",
    "        inputs=[s_shape], \n",
    "        outputs=[b_idx_2d], \n",
    "        value=helper.make_tensor(\"val\", TensorProto.FLOAT, [1], [0.0]),\n",
    "        name=\"Guideline_CreateBatchIdx\"\n",
    "    ))\n",
    "\n",
    "    # 4. Rescale boxes [N, 4] by comparing input resolution to training imgsz\n",
    "    b_scaled = \"Guideline_B_Rescaled\"\n",
    "    in_shape = \"Guideline_InputShape\"\n",
    "    graph.node.append(helper.make_node(\"Shape\", inputs=[input_name], outputs=[in_shape], name=\"Guideline_GetInShape\"))\n",
    "    \n",
    "    h_idx = \"Guideline_HIdx\"\n",
    "    w_idx = \"Guideline_WIdx\"\n",
    "    graph.initializer.extend([\n",
    "        numpy_helper.from_array(np.array(2, dtype=np.int64), name=h_idx),\n",
    "        numpy_helper.from_array(np.array(3, dtype=np.int64), name=w_idx),\n",
    "    ])\n",
    "    h_val, w_val = \"Guideline_HVal\", \"Guideline_WVal\"\n",
    "    graph.node.append(helper.make_node(\"Gather\", inputs=[in_shape, h_idx], outputs=[h_val], axis=0, name=\"Guideline_GatherH\"))\n",
    "    graph.node.append(helper.make_node(\"Gather\", inputs=[in_shape, w_idx], outputs=[w_val], axis=0, name=\"Guideline_GatherW\"))\n",
    "    \n",
    "    h_f32, w_f32 = \"Guideline_Hf32\", \"Guideline_Wf32\"\n",
    "    graph.node.append(helper.make_node(\"Cast\", inputs=[h_val], outputs=[h_f32], to=TensorProto.FLOAT, name=\"Guideline_CastH\"))\n",
    "    graph.node.append(helper.make_node(\"Cast\", inputs=[w_val], outputs=[w_f32], to=TensorProto.FLOAT, name=\"Guideline_CastW\"))\n",
    "    \n",
    "    imgsz_name = \"Guideline_TrainImgsz\"\n",
    "    graph.initializer.append(numpy_helper.from_array(np.array([float(imgsz)], dtype=np.float32), name=imgsz_name))\n",
    "    h_scale, w_scale = \"Guideline_HScale\", \"Guideline_WScale\"\n",
    "    graph.node.append(helper.make_node(\"Div\", inputs=[h_f32, imgsz_name], outputs=[h_scale], name=\"Guideline_DivH\"))\n",
    "    graph.node.append(helper.make_node(\"Div\", inputs=[w_f32, imgsz_name], outputs=[w_scale], name=\"Guideline_DivW\"))\n",
    "    \n",
    "    scales = \"Guideline_BoxScales\"\n",
    "    # Boxes are [x1, y1, x2, y2]. Scale by [W, H, W, H]\n",
    "    graph.node.append(helper.make_node(\"Concat\", inputs=[w_scale, h_scale, w_scale, h_scale], outputs=[scales], axis=0, name=\"Guideline_ConcatScales\"))\n",
    "    graph.node.append(helper.make_node(\"Mul\", inputs=[b_raw, scales], outputs=[b_scaled], name=\"Guideline_RescaleBoxes\"))\n",
    "\n",
    "    # 5. Final Concat [N, 7]\n",
    "    # Format: [x1, y1, x2, y2, score, class_id, batch_idx]\n",
    "    final_output_name = \"detections\"\n",
    "    graph.node.append(helper.make_node(\n",
    "        \"Concat\", \n",
    "        inputs=[b_scaled, s_raw, c_raw, b_idx_2d], \n",
    "        outputs=[final_output_name], \n",
    "        axis=1, \n",
    "        name=\"Guideline_FinalConcat\"\n",
    "    ))\n",
    "\n",
    "    # Update outputs to be just the consolidated detection tensor\n",
    "    del graph.output[:]\n",
    "    graph.output.append(helper.make_tensor_value_info(final_output_name, TensorProto.FLOAT, [\"N\", 7]))\n",
    "\n",
    "    onnx.checker.check_model(model)\n",
    "    onnx.save(model, onnx_path)\n",
    "    print(f\"Guideline-aligned ONNX saved to {onnx_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Preparation\n",
    "\n",
    "Download the `vertex_miap_import.csv` file, which contains GCS paths to the images and their corresponding bounding box annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import gcsfs\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# CONFIGURATION\n",
    "GCS_BUCKET = 'colin-miap-madness'\n",
    "CSV_FILENAME = 'vertex_miap_import.csv'\n",
    "GCS_CSV_PATH = f'gs://{GCS_BUCKET}/{CSV_FILENAME}'\n",
    "IMAGE_SIZE = 320\n",
    "MIN_BOX_PIXEL_SIZE = 7\n",
    "VAL_SPLIT = 0.07  # 7% for validation\n",
    "n_samples = 30000\n",
    "\n",
    "# Initialize GCS FileSystem\n",
    "fs = gcsfs.GCSFileSystem()\n",
    "\n",
    "DATASET_ROOT = '/content/datasets/miap_single_class'\n",
    "CACHE_DIR = os.path.join(DATASET_ROOT, 'image_cache')\n",
    "for d in [os.path.join(DATASET_ROOT, 'images', 'train'), \n",
    "          os.path.join(DATASET_ROOT, 'images', 'val'),\n",
    "          os.path.join(DATASET_ROOT, 'labels', 'train'), \n",
    "          os.path.join(DATASET_ROOT, 'labels', 'val'),\n",
    "          CACHE_DIR]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "print('Reading annotations CSV from GCS...')\n",
    "col_names = ['ml_use', 'gcs_path', 'label', 'x_min', 'y_min', 'c1', 'c2', 'x_max', 'y_max', 'c3', 'c4']\n",
    "df = pd.read_csv(GCS_CSV_PATH, header=None, names=col_names)\n",
    "print(f'Found {len(df)} annotations.')\n",
    "\n",
    "grouped = list(df.groupby('gcs_path'))\n",
    "random.seed(42)\n",
    "random.shuffle(grouped)\n",
    "\n",
    "# --- Native Batch Download Optimization ---\n",
    "# Collect paths for a buffer slightly larger than n_samples to account for filtering\n",
    "unique_gcs_paths = [g[0] for g in grouped[:int(n_samples * 1.5)]]\n",
    "print(f\"Preparing batch download for {len(unique_gcs_paths)} images...\")\n",
    "\n",
    "rpaths = []\n",
    "lpaths = []\n",
    "for gcs_path in unique_gcs_paths:\n",
    "    img_id = gcs_path.split('/')[-1]\n",
    "    local_path = os.path.join(CACHE_DIR, img_id)\n",
    "    if not os.path.exists(local_path):\n",
    "        rpaths.append(gcs_path)\n",
    "        lpaths.append(local_path)\n",
    "\n",
    "if rpaths:\n",
    "    print(f\"Downloading {len(rpaths)} new images using GCS batch mode...\")\n",
    "    # gcsfs.get with list inputs is a \"true batch\" operation that uses \n",
    "    # internal async connection pooling and parallel transfers.\n",
    "    fs.get(rpaths, lpaths)\n",
    "    print(\"Batch download complete.\")\n",
    "\n",
    "# --- Processing Loop ---\n",
    "images_processed = {'train': 0, 'val': 0}\n",
    "images_dropped = 0\n",
    "\n",
    "print(f'Processing and filtering images (min_box={MIN_BOX_PIXEL_SIZE}px)...')\n",
    "for gcs_path, group in tqdm(grouped):\n",
    "    split = 'val' if (images_processed['train'] + images_processed['val']) % int(1/VAL_SPLIT) == 0 else 'train'\n",
    "    \n",
    "    image_id = gcs_path.split('/')[-1] \n",
    "    cached_path = os.path.join(CACHE_DIR, image_id)\n",
    "    \n",
    "    if not os.path.exists(cached_path):\n",
    "        # Fallback for any outliers not in the original batch\n",
    "        try: fs.get(gcs_path, cached_path)\n",
    "        except: continue\n",
    "\n",
    "    local_image_path = os.path.join(DATASET_ROOT, 'images', split, image_id)\n",
    "    local_label_path = os.path.join(DATASET_ROOT, 'labels', split, image_id.replace('.jpg', '.txt'))\n",
    "\n",
    "    try:\n",
    "        with Image.open(cached_path) as img: \n",
    "            img_w, img_h = img.size\n",
    "    except Exception: \n",
    "        if os.path.exists(cached_path): os.remove(cached_path)\n",
    "        continue\n",
    "\n",
    "    yolo_labels = []\n",
    "    scale = min(IMAGE_SIZE / img_w, IMAGE_SIZE / img_h)\n",
    "    \n",
    "    for _, row in group.iterrows():\n",
    "        x_min, y_min, x_max, y_max = row['x_min'], row['y_min'], row['x_max'], row['y_max']\n",
    "        bw_px = (x_max - x_min) * img_w * scale\n",
    "        bh_px = (y_max - y_min) * img_h * scale\n",
    "        if bw_px < MIN_BOX_PIXEL_SIZE or bh_px < MIN_BOX_PIXEL_SIZE:\n",
    "            continue\n",
    "            \n",
    "        cx, cy = (x_min + x_max) / 2.0, (y_min + y_max) / 2.0\n",
    "        w, h = x_max - x_min, y_max - y_min\n",
    "        yolo_labels.append(f'0 {cx:.6f} {cy:.6f} {w:.6f} {h:.6f}')\n",
    "    \n",
    "    if yolo_labels:\n",
    "        if not os.path.exists(local_image_path):\n",
    "            os.rename(cached_path, local_image_path)\n",
    "        with open(local_label_path, 'w') as f: f.write('\\n'.join(yolo_labels))\n",
    "        images_processed[split] += 1\n",
    "    else:\n",
    "        images_dropped += 1\n",
    "        if os.path.exists(cached_path): os.remove(cached_path)\n",
    "        \n",
    "    if images_processed[\"train\"] >= n_samples:\n",
    "        break\n",
    "\n",
    "print('\\n--- Data Preparation Summary ---')\n",
    "print(f\"Training images: {images_processed['train']}\")\n",
    "print(f\"Validation images: {images_processed['val']}\")\n",
    "print(f'Images dropped (no valid boxes): {images_dropped}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Dataset YAML File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "dataset_yaml_path = os.path.join(DATASET_ROOT, 'data.yaml')\n",
    "yaml_content = {\n",
    "    'path': os.path.abspath(DATASET_ROOT),\n",
    "    'train': 'images/train',\n",
    "    'val': 'images/val',\n",
    "    'names': {0: 'person'}\n",
    "}\n",
    "with open(dataset_yaml_path, 'w') as f: yaml.dump(yaml_content, f)\n",
    "print(f'Dataset YAML created at: {dataset_yaml_path}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "MODEL_VARIANT = 'yolo26n.pt' \n",
    "\n",
    "# Check if a resume checkpoint exists on Drive to handle training crashes\n",
    "last_ckpt_drive = os.path.join(GDRIVE_SAVE_DIR, 'last.pt')\n",
    "if os.path.exists(last_ckpt_drive):\n",
    "    print(f\"Found existing checkpoint on Drive. Resuming from: {last_ckpt_drive}\")\n",
    "    model = YOLO(last_ckpt_drive)\n",
    "    resume_arg = True\n",
    "else:\n",
    "    model = YOLO(MODEL_VARIANT)\n",
    "    resume_arg = False\n",
    "\n",
    "# --- NEW: Callback to sync weights to Drive during training ---\n",
    "def on_train_epoch_end(trainer):\n",
    "    \"\"\"Callback to sync best and last checkpoints to Google Drive after each epoch.\"\"\"\n",
    "    best_local = os.path.join(trainer.save_dir, 'weights', 'best.pt')\n",
    "    last_local = os.path.join(trainer.save_dir, 'weights', 'last.pt')\n",
    "    \n",
    "    # Sync best weights\n",
    "    if os.path.exists(best_local):\n",
    "        shutil.copy2(best_local, os.path.join(GDRIVE_SAVE_DIR, 'best.pt'))\n",
    "    \n",
    "    # Sync last weights (for resume capability)\n",
    "    if os.path.exists(last_local):\n",
    "        shutil.copy2(last_local, os.path.join(GDRIVE_SAVE_DIR, 'last.pt'))\n",
    "\n",
    "# Register the callback before starting training\n",
    "model.add_callback(\"on_train_epoch_end\", on_train_epoch_end)\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "results = model.train(\n",
    "    data=dataset_yaml_path, \n",
    "    imgsz=IMAGE_SIZE, \n",
    "    epochs=30, \n",
    "    batch=32, \n",
    "    name='miap_person_detector',\n",
    "    project='runs',\n",
    "    resume=resume_arg\n",
    ")\n",
    "print('\\nTraining complete!')\n",
    "\n",
    "# Final weights already synced by callback, but confirming location\n",
    "best_pt_drive = os.path.join(GDRIVE_SAVE_DIR, 'best.pt')\n",
    "print(f'Best weights available on Drive: {best_pt_drive}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Export to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the best weights from Google Drive for Export/Quantization\n",
    "best_weights_path = os.path.join(GDRIVE_SAVE_DIR, 'best.pt')\n",
    "print(f\"Loading weights from Drive: {best_weights_path}\")\n",
    "model = YOLO(best_weights_path)\n",
    "\n",
    "print('\\n1. Exporting Raw FP32 ONNX model with integrated NMS...')\n",
    "# nms=True adds the NMS node. opset=17 is recommended by guidelines.\n",
    "# max_det=100 limits the number of detections to reduce overhead.\n",
    "fp32_raw_path = model.export(format='onnx', imgsz=IMAGE_SIZE, opset=17, simplify=True, nms=True, max_det=100)\n",
    "\n",
    "print('\\n2. Embedding Preprocessing (uint8 -> float32 -> resize) into model...')\n",
    "fp32_pre_path = fp32_raw_path.replace('.onnx', '_pre_u8.onnx')\n",
    "embed_uint8_preprocess_into_onnx(fp32_raw_path, fp32_pre_path, IMAGE_SIZE)\n",
    "\n",
    "print('\\n3. Finalizing ONNX for deployment (aligning with GUIDELINES)...')\n",
    "# This function rescales coordinates to uint8 input size and consolidates to [N, 7]\n",
    "finalize_onnx_for_deployment(fp32_pre_path, IMAGE_SIZE)\n",
    "print(f'FP32 ONNX (guideline aligned) saved to: {fp32_pre_path}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Advanced Static Quantization (Improved)\n",
    "\n",
    "YOLO models can be sensitive to static quantization. We use ONNX Runtimeâ€™s advanced quantization tools directly, using **Entropy (KL Divergence)** calibration and a larger calibration set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "class MIAPCalibrationDataReader(CalibrationDataReader):\n",
    "    def __init__(self, image_dir, imgsz, max_images=1000):\n",
    "        self.image_paths = glob.glob(os.path.join(image_dir, '*.jpg'))\n",
    "        random.shuffle(self.image_paths)\n",
    "        self.image_paths = self.image_paths[:max_images]\n",
    "        self.imgsz = imgsz\n",
    "        self.index = 0\n",
    "        \n",
    "        # Get input name from model\n",
    "        session = ort.InferenceSession(fp32_pre_path, providers=['CPUExecutionProvider'])\n",
    "        self.input_name = session.get_inputs()[0].name\n",
    "\n",
    "    def get_next(self):\n",
    "        if self.index >= len(self.image_paths): return None\n",
    "        \n",
    "        # Load and resize to exact imgsz in uint8 to match the new uint8 input\n",
    "        img = Image.open(self.image_paths[self.index]).convert('RGB')\n",
    "        img = img.resize((self.imgsz, self.imgsz), Image.BILINEAR)\n",
    "        input_data = np.array(img).transpose(2, 0, 1)[None, ...].astype(np.uint8)\n",
    "        \n",
    "        self.index += 1\n",
    "        return {self.input_name: input_data}\n",
    "\n",
    "print('Starting Static Quantization...')\n",
    "# Calibrate on a representative subset of the training data (e.g. 1000-2000 images is usually plenty)\n",
    "# but we can go higher if desired. 2000 is a good balance.\n",
    "dr = MIAPCalibrationDataReader(os.path.join(DATASET_ROOT, 'images', 'train'), IMAGE_SIZE, max_images=2000)\n",
    "\n",
    "int8_path = fp32_pre_path.replace('.onnx', '_int8.onnx')\n",
    "\n",
    "quantize_static(\n",
    "    model_input=fp32_pre_path, \n",
    "    model_output=int8_path, \n",
    "    calibration_data_reader=dr, \n",
    "    quant_format=QuantFormat.QDQ,\n",
    "    activation_type=QuantType.QUInt8, \n",
    "    weight_type=QuantType.QInt8, \n",
    "    per_channel=True, \n",
    "    reduce_range=False, # Often better for accuracy on non-Intel hardware\n",
    "    calibrate_method=CalibrationMethod.Entropy # KL Divergence - better for fine gradients\n",
    ")\n",
    "\n",
    "print(f'\\nINT8 ONNX model saved to: {int8_path}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Save & Inspect ONNX Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "\n",
    "def inspect_onnx_model(model_path):\n",
    "    print(f'\\n--- Inspecting: {os.path.basename(model_path)} ---')\n",
    "    # Using CPU provider for inspection\n",
    "    sess = ort.InferenceSession(model_path, providers=['CPUExecutionProvider'])\n",
    "    input_nodes = sess.get_inputs()\n",
    "    print(f'Inputs: {[(n.name, n.shape, n.type) for n in input_nodes]}')\n",
    "    print(f'Outputs: {[(n.name, n.shape, n.type) for n in sess.get_outputs()]}')\n",
    "    \n",
    "    # Use global IMAGE_SIZE or default to 320 if not found\n",
    "    img_sz = IMAGE_SIZE if 'IMAGE_SIZE' in globals() else 320\n",
    "    \n",
    "    # Test with dummy input - use case-insensitive check for uint8\n",
    "    if \"uint8\" in input_nodes[0].type.lower():\n",
    "        dummy_input = np.random.randint(0, 255, size=(1, 3, img_sz, img_sz), dtype=np.uint8)\n",
    "    else:\n",
    "        dummy_input = np.random.rand(1, 3, img_sz, img_sz).astype(np.float32)\n",
    "        \n",
    "    outputs = sess.run(None, {input_nodes[0].name: dummy_input})\n",
    "    print(f'Output shapes: {[o.shape for o in outputs]}')\n",
    "\n",
    "# Define final paths in Drive\n",
    "final_fp32 = os.path.join(GDRIVE_SAVE_DIR, os.path.basename(fp32_pre_path))\n",
    "final_int8 = os.path.join(GDRIVE_SAVE_DIR, os.path.basename(int8_path))\n",
    "\n",
    "print(f\"Saving FP32 ONNX model to: {final_fp32}\")\n",
    "shutil.copy2(fp32_pre_path, final_fp32)\n",
    "\n",
    "print(f\"Saving INT8 ONNX model to: {final_int8}\")\n",
    "shutil.copy2(int8_path, final_int8)\n",
    "\n",
    "print(f'\\nAll models saved to Drive: {GDRIVE_SAVE_DIR}')\n",
    "inspect_onnx_model(final_fp32)\n",
    "inspect_onnx_model(final_int8)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
