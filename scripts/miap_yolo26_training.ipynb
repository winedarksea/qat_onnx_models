{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLO26 Nano Training for MIAP Person Detection\n",
    "\n",
    "This notebook performs the following steps:\n",
    "1.  **Setup**: Installs necessary libraries and connects to Google Drive and Google Cloud for data access.\n",
    "2.  **Data Preparation**: \n",
    "    - Downloads annotations for the MIAP (person subset of Google Open Dataset) from a GCS bucket.\n",
    "    - Downloads the corresponding images from the public Open Images dataset bucket.\n",
    "    - Filters out images containing only very small bounding boxes (width or height < 6 pixels).\n",
    "    - Converts the dataset into the YOLO format required by Ultralytics.\n",
    "3.  **Training**: Trains a `yolo26n` (nano) model on the prepared dataset.\n",
    "4.  **Export**: \n",
    "    - Exports the trained model to ONNX format, including preprocessing and NMS post-processing.\n",
    "    - Creates a Float32 version.\n",
    "    - Creates an INT8 quantized version using static calibration.\n",
    "5.  **Save & Verify**: Saves the final models to Google Drive and runs a quick verification with ONNX Runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ultralytics onnx onnxruntime onnxsim pandas gcsfs tqdm -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup and Authentication\n",
    "\n",
    "Mount Google Drive to save the final models and authenticate with Google Cloud to access the dataset annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive, auth\n",
    "import os\n",
    "\n",
    "print(\"Mounting Google Drive...\")\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(\"Authenticating with Google Cloud...\")\n",
    "auth.authenticate_user()\n",
    "\n",
    "# Define a directory in your Google Drive to save the models\n",
    "GDRIVE_SAVE_DIR = '/content/drive/MyDrive/miap_yolov26_models'\n",
    "os.makedirs(GDRIVE_SAVE_DIR, exist_ok=True)\n",
    "print(f\"Models will be saved to: {GDRIVE_SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import numpy as np\n",
    "from onnx import helper, numpy_helper, TensorProto\n",
    "import onnxruntime as ort\n",
    "from onnxruntime.quantization import quantize_static, CalibrationDataReader, QuantFormat, QuantType, CalibrationMethod\n",
    "\n",
    "def embed_uint8_preprocess_into_onnx(in_onnx, out_onnx, imgsz, input_scale=1.0/255.0):\n",
    "    \"\"\"\n",
    "    Embeds uint8 normalization and resize into the ONNX graph.\n",
    "    Input: [1, 3, H, W] uint8\n",
    "    Output: [1, 3, imgsz, imgsz] float32\n",
    "    \"\"\"\n",
    "    model = onnx.load(in_onnx)\n",
    "    orig_in_name = model.graph.input[0].name\n",
    "    \n",
    "    new_input_name = \"images_uint8\"\n",
    "    new_input = helper.make_tensor_value_info(new_input_name, TensorProto.UINT8, [1, 3, \"H\", \"W\"])\n",
    "    \n",
    "    cast_out = f\"{orig_in_name}__cast_f32\"\n",
    "    scaled_out = f\"{orig_in_name}__scaled\"\n",
    "    resize_out = orig_in_name\n",
    "\n",
    "    nodes = [\n",
    "        helper.make_node(\"Cast\", inputs=[new_input_name], outputs=[cast_out], to=TensorProto.FLOAT),\n",
    "        helper.make_node(\"Mul\", inputs=[cast_out, \"Preprocess_Scale_Val\"], outputs=[scaled_out])\n",
    "    ]\n",
    "    \n",
    "    # Preprocessing Initializers\n",
    "    scale_tensor = numpy_helper.from_array(np.array([input_scale], dtype=np.float32), name=\"Preprocess_Scale_Val\")\n",
    "    roi = numpy_helper.from_array(np.array([], dtype=np.float32), name=\"Preprocess_ROI\")\n",
    "    scales = numpy_helper.from_array(np.array([], dtype=np.float32), name=\"Preprocess_Scales\")\n",
    "    sizes = numpy_helper.from_array(np.array([1, 3, imgsz, imgsz], dtype=np.int64), name=\"Preprocess_Sizes\")\n",
    "    \n",
    "    model.graph.initializer.extend([scale_tensor, roi, scales, sizes])\n",
    "    \n",
    "    nodes.append(helper.make_node(\"Resize\", \n",
    "                                  inputs=[scaled_out, \"Preprocess_ROI\", \"Preprocess_Scales\", \"Preprocess_Sizes\"], \n",
    "                                  outputs=[resize_out], mode=\"linear\"))\n",
    "\n",
    "    # Reconstruct graph\n",
    "    del model.graph.input[0]\n",
    "    model.graph.input.insert(0, new_input)\n",
    "    model.graph.node[:0] = nodes\n",
    "    \n",
    "    onnx.checker.check_model(model)\n",
    "    onnx.save(model, out_onnx)\n",
    "    print(f\"Preprocessed ONNX saved to {out_onnx}\")\n",
    "    return out_onnx\n",
    "\n",
    "def fix_onnx_outputs(onnx_path):\n",
    "    \"\"\"Renames outputs for consistency.\"\"\"\n",
    "    model = onnx.load(onnx_path)\n",
    "    for i, out in enumerate(model.graph.output):\n",
    "        if \"output\" in out.name or i == 0:\n",
    "            out.name = \"detections\"\n",
    "    # Update nodes producing these outputs\n",
    "    for node in model.graph.node:\n",
    "        for i, name in enumerate(node.output):\n",
    "            if \"output\" in name: node.output[i] = \"detections\"\n",
    "    onnx.save(model, onnx_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Preparation\n",
    "\n",
    "Download the `vertex_miap_import.csv` file, which contains GCS paths to the images and their corresponding bounding box annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import gcsfs\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# CONFIGURATION\n",
    "GCS_BUCKET = 'colin-miap-madness'\n",
    "CSV_FILENAME = 'vertex_miap_import.csv'\n",
    "GCS_CSV_PATH = f'gs://{GCS_BUCKET}/{CSV_FILENAME}'\n",
    "IMAGE_SIZE = 320\n",
    "MIN_BOX_PIXEL_SIZE = 6\n",
    "VAL_SPLIT = 0.07  # 7% for validation\n",
    "n_samples = 10000\n",
    "\n",
    "# Initialize GCS FileSystem (uses default project/auth from Colab environment)\n",
    "fs = gcsfs.GCSFileSystem()\n",
    "\n",
    "DATASET_ROOT = '/content/datasets/miap_single_class'\n",
    "for split in ['train', 'val']:\n",
    "    os.makedirs(os.path.join(DATASET_ROOT, 'images', split), exist_ok=True)\n",
    "    os.makedirs(os.path.join(DATASET_ROOT, 'labels', split), exist_ok=True)\n",
    "\n",
    "print('Reading annotations CSV from GCS...')\n",
    "col_names = ['ml_use', 'gcs_path', 'label', 'x_min', 'y_min', 'c1', 'c2', 'x_max', 'y_max', 'c3', 'c4']\n",
    "df = pd.read_csv(GCS_CSV_PATH, header=None, names=col_names)\n",
    "print(f'Found {len(df)} annotations.')\n",
    "\n",
    "grouped = list(df.groupby('gcs_path'))\n",
    "random.seed(42)\n",
    "random.shuffle(grouped)\n",
    "\n",
    "images_processed = {'train': 0, 'val': 0}\n",
    "images_dropped = 0\n",
    "\n",
    "print(f'Processing and filtering images (min_box={MIN_BOX_PIXEL_SIZE}px)...')\n",
    "for gcs_path, group in tqdm(grouped):\n",
    "    split = 'val' if (images_processed['train'] + images_processed['val']) % int(1/VAL_SPLIT) == 0 else 'train'\n",
    "    \n",
    "    image_id = gcs_path.split('/')[-1].replace('.jpg', '')\n",
    "    # http_url = f'https://storage.googleapis.com/open-images-dataset/train/{image_id}.jpg'\n",
    "    local_image_path = os.path.join(DATASET_ROOT, 'images', split, f'{image_id}.jpg')\n",
    "    local_label_path = os.path.join(DATASET_ROOT, 'labels', split, f'{image_id}.txt')\n",
    "\n",
    "    if not os.path.exists(local_image_path):\n",
    "        try:\n",
    "            # Pull directly from GCS bucket to stay on internal network\n",
    "            # urrllib.request.urlretrieve(http_url, local_image_path)\n",
    "            fs.get(gcs_path, local_image_path)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    try:\n",
    "        with Image.open(local_image_path) as img: img_w, img_h = img.size\n",
    "    except Exception: \n",
    "        if os.path.exists(local_image_path): os.remove(local_image_path)\n",
    "        continue\n",
    "\n",
    "    yolo_labels = []\n",
    "    # YOLO requires scale factor for small box filtering to match target imgsz\n",
    "    scale = min(IMAGE_SIZE / img_w, IMAGE_SIZE / img_h)\n",
    "    \n",
    "    for _, row in group.iterrows():\n",
    "        x_min, y_min, x_max, y_max = row['x_min'], row['y_min'], row['x_max'], row['y_max']\n",
    "        \n",
    "        # Filter small boxes\n",
    "        bw_px = (x_max - x_min) * img_w * scale\n",
    "        bh_px = (y_max - y_min) * img_h * scale\n",
    "        if bw_px < MIN_BOX_PIXEL_SIZE or bh_px < MIN_BOX_PIXEL_SIZE:\n",
    "            continue\n",
    "            \n",
    "        cx, cy = (x_min + x_max) / 2.0, (y_min + y_max) / 2.0\n",
    "        w, h = x_max - x_min, y_max - y_min\n",
    "        yolo_labels.append(f'0 {cx:.6f} {cy:.6f} {w:.6f} {h:.6f}')\n",
    "    \n",
    "    if yolo_labels:\n",
    "        with open(local_label_path, 'w') as f: f.write('\\n'.join(yolo_labels))\n",
    "        images_processed[split] += 1\n",
    "    else:\n",
    "        if os.path.exists(local_image_path): os.remove(local_image_path)\n",
    "        images_dropped += 1\n",
    "    if images_processed[\"train\"] == n_samples:\n",
    "        break\n",
    "\n",
    "print('\\n--- Data Preparation Summary ---')\n",
    "print(f\"Training images: {images_processed['train']}\")\n",
    "print(f\"Validation images: {images_processed['val']}\")\n",
    "print(f'Images dropped (no valid boxes): {images_dropped}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Dataset YAML File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "dataset_yaml_path = os.path.join(DATASET_ROOT, 'data.yaml')\n",
    "yaml_content = {\n",
    "    'path': os.path.abspath(DATASET_ROOT),\n",
    "    'train': 'images/train',\n",
    "    'val': 'images/val',\n",
    "    'names': {0: 'person'}\n",
    "}\n",
    "with open(dataset_yaml_path, 'w') as f: yaml.dump(yaml_content, f)\n",
    "print(f'Dataset YAML created at: {dataset_yaml_path}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "MODEL_VARIANT = 'yolo26n.pt' \n",
    "\n",
    "model = YOLO(MODEL_VARIANT)\n",
    "results = model.train(\n",
    "    data=dataset_yaml_path, \n",
    "    imgsz=IMAGE_SIZE, \n",
    "    epochs=50, \n",
    "    batch=32, \n",
    "    name='miap_person_detector',\n",
    "    project='runs'\n",
    ")\n",
    "print('\\nTraining complete!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Export to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from ultralytics import YOLO\n",
    "\n",
    "best_weights_path = os.path.join(results.save_dir, 'weights/best.pt')\n",
    "model = YOLO(best_weights_path)\n",
    "\n",
    "print('\\n1. Exporting Raw FP32 ONNX model...')\n",
    "# imgsz is passed as a list/tuple to ensure output match\n",
    "fp32_raw_path = model.export(format='onnx', imgsz=IMAGE_SIZE, opset=17, simplify=True)\n",
    "\n",
    "print('\\n2. Embedding Preprocessing (uint8 -> float32 -> resize) into model...')\n",
    "fp32_pre_path = fp32_raw_path.replace('.onnx', '_pre_u8.onnx')\n",
    "embed_uint8_preprocess_into_onnx(fp32_raw_path, fp32_pre_path, IMAGE_SIZE)\n",
    "\n",
    "print('\\n3. Finalizing FP32 model labels...')\n",
    "fix_onnx_outputs(fp32_pre_path)\n",
    "print(f'FP32 ONNX (with preprocessing) saved to: {fp32_pre_path}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Advanced Static Quantization (Improved)\n",
    "\n",
    "YOLO models can be sensitive to static quantization. We use ONNX Runtimeâ€™s advanced quantization tools directly, using **Entropy (KL Divergence)** calibration and a larger calibration set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "class MIAPCalibrationDataReader(CalibrationDataReader):\n",
    "    def __init__(self, image_dir, imgsz, max_images=1000):\n",
    "        self.image_paths = glob.glob(os.path.join(image_dir, '*.jpg'))\n",
    "        random.shuffle(self.image_paths)\n",
    "        self.image_paths = self.image_paths[:max_images]\n",
    "        self.imgsz = imgsz\n",
    "        self.index = 0\n",
    "        \n",
    "        # Get input name from model\n",
    "        session = ort.InferenceSession(fp32_pre_path, providers=['CPUExecutionProvider'])\n",
    "        self.input_name = session.get_inputs()[0].name\n",
    "\n",
    "    def get_next(self):\n",
    "        if self.index >= len(self.image_paths): return None\n",
    "        \n",
    "        # Load and resize to exact imgsz in uint8 to match the new uint8 input\n",
    "        img = Image.open(self.image_paths[self.index]).convert('RGB')\n",
    "        img = img.resize((self.imgsz, self.imgsz), Image.BILINEAR)\n",
    "        input_data = np.array(img).transpose(2, 0, 1)[None, ...].astype(np.uint8)\n",
    "        \n",
    "        self.index += 1\n",
    "        return {self.input_name: input_data}\n",
    "\n",
    "print('Starting Static Quantization...')\n",
    "# Calibrate on a representative subset of the training data (e.g. 1000-2000 images is usually plenty)\n",
    "# but we can go higher if desired. 2000 is a good balance.\n",
    "dr = MIAPCalibrationDataReader(os.path.join(DATASET_ROOT, 'images', 'train'), IMAGE_SIZE, max_images=2000)\n",
    "\n",
    "int8_path = fp32_pre_path.replace('.onnx', '_int8.onnx')\n",
    "\n",
    "quantize_static(\n",
    "    model_input=fp32_pre_path, \n",
    "    model_output=int8_path, \n",
    "    calibration_data_reader=dr, \n",
    "    quant_format=QuantFormat.QDQ,\n",
    "    activation_type=QuantType.QUInt8, \n",
    "    weight_type=QuantType.QInt8, \n",
    "    per_channel=True, \n",
    "    reduce_range=False, # Often better for accuracy on non-Intel hardware\n",
    "    calibrate_method=CalibrationMethod.Entropy # KL Divergence - better for fine gradients\n",
    ")\n",
    "\n",
    "print(f'\\nINT8 ONNX model saved to: {int8_path}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Save & Inspect ONNX Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_onnx_model(model_path):\n",
    "    print(f'\\n--- Inspecting: {os.path.basename(model_path)} ---')\n",
    "    sess = ort.InferenceSession(model_path, providers=['CPUExecutionProvider'])\n",
    "    input_nodes = sess.get_inputs()\n",
    "    print(f'Inputs: {[(n.name, n.shape, n.type) for n in input_nodes]}')\n",
    "    print(f'Outputs: {[(n.name, n.shape, n.type) for n in sess.get_outputs()]}')\n",
    "    \n",
    "    # Test with dummy uint8 input if needed\n",
    "    if \"Uint8\" in input_nodes[0].type:\n",
    "        dummy_input = np.random.randint(0, 255, size=(1, 3, IMAGE_SIZE, IMAGE_SIZE), dtype=np.uint8)\n",
    "    else:\n",
    "        dummy_input = np.random.rand(1, 3, IMAGE_SIZE, IMAGE_SIZE).astype(np.float32)\n",
    "        \n",
    "    outputs = sess.run(None, {input_nodes[0].name: dummy_input})\n",
    "    print(f'Output shapes: {[o.shape for o in outputs]}')\n",
    "\n",
    "# Copy results to Drive\n",
    "final_fp32 = os.path.join(GDRIVE_SAVE_DIR, os.path.basename(fp32_pre_path))\n",
    "final_int8 = os.path.join(GDRIVE_SAVE_DIR, os.path.basename(int8_path))\n",
    "\n",
    "shutil.copy2(fp32_pre_path, final_fp32)\n",
    "shutil.copy2(int8_path, final_int8)\n",
    "\n",
    "print(f'\\nModels saved to Drive: {GDRIVE_SAVE_DIR}')\n",
    "inspect_onnx_model(final_fp32)\n",
    "inspect_onnx_model(final_int8)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
